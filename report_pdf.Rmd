---
title: "Projectreport Machine Learning 2"
header-includes:
  - \usepackage[english]{babel}
  - \DeclareUnicodeCharacter{308}{oe}  
  #- \usepackage{bbm}
author: "Maluna Menke, Ari (Sara) Wahl, Pavlo Kravets"
date: "`r Sys.Date()`"

output:
  #html_document: 
  pdf_document:
    
    toc: true
    toc_depth: 6
  start-page: 2
---

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic} 
\centering
\raggedright
\newpage

---

```{r setup, include=FALSE}
params <- list(html=knitr::is_html_output())
knitr::opts_chunk$set(echo = params$html)
rm(list = ls(all.names = TRUE)) # reset the environment

#names(RISK)

source("Setup.R")
```

### 1. Introduction
\

Our general idea was to work with LGBT-related data. This was not as easy as expected, since it seems there are not a lot of datasets openly available that have that kind of information. Finally, we found a US survey by the CDC, that regularly monitors the country's youth in a lot of dimensions, but among other questions also asks for sexual experiences and identification.   

### 2. The Dataset
\

"The *[Youth Risk Behavior Survey (YRBS)](https://www.cdc.gov/healthyyouth/data/yrbs/data.htm)* measures health-related behaviors and experiences that can lead to death and disability among youth and adults.[...] Some of the health-related behaviors and experiences monitored are:
- Student demographics: sex, sexual identity, race and ethnicity, and grade
- Youth health behaviors and conditions: sexual, injury and violence, bullying, diet and physical activity, obesity, and mental health, including suicide
- Substance use behaviors: electronic vapor product and tobacco product use, alcohol use, and other drug use
- Student experiences: parental monitoring, school connectedness, unstable housing, and exposure to community violence [[1]](#1).
It is a national survey conducted by the CDC (Center for Disease Control and Prevention) and includes high school students from both private and public schools within the U.S. Data is collected from 1991 through 2021, we are only using the most recent data from 2021. If you want to learn more about the data there is an accompanying Data User Guide.[[2]](#2).


#### 2.1 Preprocessing of the dataset
\

```{r summary, include = FALSE}
# possible summaries of the data:
# summary(RISK) 
# describe(RISK) # more detail
```

To preprocess the dataset, we first ran a summary of our dataset. The number of NAs seems to depend very much on the question. The variable "orig_rec" only contained NAs and has therefore been removed, as well as the variable "site" which only contained "XX" entries. Variables q4 and q5 are already aggregated in "raceeth" and have also been deleted.
The variable "record" seems to be an ID for the observations. This has to be considered later. 

#### 2.2 Missing Data

```{r variables overview, include =FALSE}
na_ratio<-sum(is.na(RISK)==TRUE)/(dim(RISK)[1]*dim(RISK)[2])# ratio of NAs among all datapoints
print(na_ratio*100) # % percentage of NAs in data
sum(is.na(RISK)==TRUE) # sum of NAs in dataset
```
\

We will first exclude all the observations with NAs in all the target-related variables q25 to q29. Since we want to build our target variable on these questions, the target variable cannot all be empty. The amount of data available should be enough to just exclude these observations. After removing the observations that have NAs in all the variables, that are used to create our target variable, we still have around 13.7% NAs in the dataset.
\
What if we had just excluded every NA in the dataset? We will try and see if this is a viable option, since this woul not just be quick and easy, but we would also just have "real" answers. The exclusion of NAs leads to a severe reduction in the number of observations. The original data consisted of 17232 observations, after reducing the target-related NAs only, we have 11753 observations left. If we omit all NAs, the reduced dataframe still has 4334 observations.
\

```{r check exclude NAs, include=FALSE}
RISK_no_NAs <- na.omit(RISK)
summary(RISK_no_NAs)
```

In this case need to assess the loss of information foremost about our target variable. The important question is if there is a pattern to the missingness in our data, not just, but especially about our target variable.

```{r Tables for Missingness in q28, include=FALSE}
table(RISK_no_NAs$q28)
table(RISK$q28)

table(RISK_no_NAs$q28)/4334
table(RISK$q28)/17138
sum(is.na(RISK$q28)==TRUE)/17138
```

#### 2.2.2 Omitting NAs vs Data Imputation
\

If we can omit the NAs or if it may be necessary to impute the missing data points, depends on the type of missingness. If data is missing completely at random (MCAR), we can omit the NAs, if it is just missing at random (MAR) we would rather impute the data. If the data is missing not at random (MNAR), it would be a quite difficult problem because we cannot easily impute the missing data then. To find out if we can just omit the data, an MCAR test was applied. 
\
We test the target-related variables q25 to q29 for potential pattern(s) in the missing data. This results in a p-value of 0, which means we can say for sure, that the data is not missing completely at random. Just omitting all NAs could be problematic and lead to bias.
\

```{r MCAR test for target columns}
result <- mcar_test(RISK[, c("q25", "q26", "q27", "q28", "q29")])
print(result)
```
Because of this, we will use a rule base approach to create the target variable and impute the predictive variables afterwards. To ensure a good imputation, we need to impute our NAs before reducing the dataset to 2000 observations. To run the imputation properly we need to factorize our nominal and ordinal variables first.
```{r analyse missingness, include=FALSE, eval=FALSE}
plot_missing(RISK[1:20])
plot_missing(RISK[20:40])
plot_missing(RISK[40:60])
plot_missing(RISK[60:80])
plot_missing(RISK[80:100])
plot_missing(RISK)
```


#### 2.3 Target Variable
\

As a target variable, we decided to calculate a score from 5 questions that reflect the suicide risk of the person (observation) in question. This score is aggregated with a rule-based approach. 

```{r creating the target variable, include= FALSE, eval=FALSE}
RISK_num<-RISK
RISK_num$q25 <- as.numeric(RISK_num$q25)
RISK_num$q26 <- as.numeric(RISK_num$q26)
RISK_num$q27 <- as.numeric(RISK_num$q27)
RISK_num$q28 <- as.numeric(RISK_num$q28)
RISK_num$q29 <- as.numeric(RISK_num$q29)
RISK_s5 <- RISK_num %>% 
  mutate(suicidal_class = ifelse(q29 > 1,   5, # 3
                          ifelse(q28 > 1,   5, # 3
                          ifelse(q27 == 1,  4, # 3
                          ifelse(q26 == 1,  3, # 2
                          ifelse(q25 == 1,  2,
                                            1))))))

RISK_s5[, c("q25", "q26", "q27", "q28", "q29")] <- NULL  # removing the columns related to the target variable

RISK_s5$suicidal_class<-factor(RISK_s5$suicidal_class, ordered = TRUE)  # factorize the target variable
```

\ 
After creating the target variable we need to exclude the variables q25 to q29, which were used for creating it, from our dataset. 

#### 2.4 Imputation


```{r imputing, include= FALSE, eval=FALSE}
set.seed(0)
# the categorical variables are factorized in the setup.R file
RISK_imp_s5 <- missRanger(RISK_s5, pmm.k = 200, num.trees = 500, sample.fraction = 1)
```


#### 2.5 Reducing and balancing the dataset to 2000 observations
\

We need to reduce our data to the maximum allowed size of 2000 observations. To ensure the best possible data quality, we want to ensure that our dataset is balanced. Intuitively, we are considering if it is best to still use as much of the non-imputed data for our smaller dataset as possible, before filling it up with imputed data, since non-imputed data is usually of better quality. On the other hand the data seemingly shows patterns in the missingness so there are reasons to just do a stratified sampling over the imputed data as well. To do a proper statified sampling we need to identify the stratification variables. Therefore we will calculate the correlations with the target variable and see which variables are highly correlated to our target variable. These will then as well as the target variable be used as stratification variables.

```{r stratified sampling, include = FALSE, eval=FALSE}
# this code snipped was produced with the help of AI
# *
size_per_class <- rep(400, times = 5) # 400 samples from each of 5 classes, adjust as needed
sample_obj <- strata(RISK_imp_s5, stratanames = "suicidal_class", size = size_per_class, method = "srswor")
RISK_2000_5 <- getdata(RISK_imp_s5, sample_obj)
# *
# clean up:
# rm(RISK, RISK_imp_num, RISK_imp, sample_obj, strat_df)  # remove unnecessary variables in the environment

```



#### 2.6 Simple Synopsis of the Dataset



```{r RISK_2000_5 load and inspect, include=FALSE}
source("Setup_RISK.R")

variable_types <- skim(RISK_2000_5) # get the variable types
summary(variable_types) # summary on variable types
# run lines below separately again to show: (command+enter)
table(variable_types$factor.ordered) # summary on factor variables
variable_types[variable_types$skim_type=="numeric",]
# introduce(RISK_2000_5)
```
\

number of observations: 2000 
number of variables: 100

datatypes:
factor: 96
- nominal variables: 29
- ordinal variables: 67
numeric variables: 4
- discrete variables: 96 (here all factor variables)
- continuous variables: 4 (here all numeric variables)


### 3. Additional Data Preparation

question: would it introduce information leakage to reduce the features before splitting the data?

```{r check for constant or near zero variance features and eliminate them, include=FALSE}
# ** this codeblock was done using AI **
# check if there are constant features
constantFeatures <- sapply(RISK_2000_5, function(x) {
    if(is.factor(x) || is.character(x)) {
        return(length(unique(x)) == 1)
    } else {
        return(var(x, na.rm = TRUE) == 0)
    }
})

constantFeatureNames <- names(RISK_2000_5)[constantFeatures]

# Print constant features
print(constantFeatureNames)

# Check for near-zero variance features
nzvFeatures <- nearZeroVar(RISK_2000_5, saveMetrics = TRUE)
nzvFeatureNames <- names(RISK_2000_5)[nzvFeatures$nzv]
if(any(nzvFeatures$nzv)) {
    print("Near-zero variance features found:")
    print(names(RISK_2000_5)[nzvFeatures$nzv])
} else {
    print("No near-zero variance features.")
}

# Remove near-zero variance features
RISK_reduced <- RISK_2000_5[, !names(RISK_2000_5) %in% nzvFeatureNames]

# ** end of AI produced code block **
rm(nzvFeatures, constantFeatureNames, constantFeatures, nzvFeatureNames)
```


#### 3.1 Feature Reduction
\

Since our dataset has lots of variables, we decided to start by excluding some variables depending on the estimated feature importance.

#### 3.1.1 Correlations 
\ 

Unfortunately at this point we have to many variables to do a pairs plot or correlation plot with a visually usable outcome. We will therefore perform a correlation analysis only with respect to the target variable and in numeric format instead of any visual plot.

```{r, corr plot, echo=FALSE, include=FALSE}
# plot_correlation(RISK_2000_5)
#pairs(RISK) # -> non numeric arguments included
#plot(RISK) 

# corr_mat=cor(RISK, method="s") #create spearman correlation matrix

#corrplot(corr_mat, method = "color",
#     type = "upper", order = "hclust", 
#     tl.col = "black") 

# preprocessing for correlations:
RISK_num <- as.data.frame(lapply(RISK_reduced, as.numeric)) # convert all variables to numeric
corr_mat=cor(RISK_num, method="s") # correlation matrix
target_corr <- corr_mat[, "suicidal_class"] # get correlations with respect to the target variable
# df with variable names and their correlation values:
corr_data <- data.frame(
  var = names(target_corr),
  corr = target_corr
)
 
rm(RISK_num, target_corr, corr_mat) # not needed any more

# Sort the data frame by correlation values
sorted_corr_data <- corr_data[order(-corr_data$corr), ]

# stratification variable vector:
corr_vars <- sorted_corr_data[abs(sorted_corr_data$corr) > 0.25,] 
# select all correlations as possible strata (pos & neg) bigger than 0.25


# all variables highly correlated with target variable
RISK_red<-RISK_reduced[, corr_vars$var] 
# correlations of these
plot_correlation(RISK_red)
rm(sorted_corr_data, corr_data)
```


```{r correlations with the target, include=FALSE}
print(corr_vars)
```

The 18 variables with high correlations (>0.25) with our target variable are:
\
q93                 
q85                     
q45           
q35          
q36               
q39                  
q64                 
q46               
q20                   
q30                     
q24                     
q19                       
q98                       
q34 
q41
q43
q44
q47



#### 3.1.2 Feature Selection Algorithm
\

Since the data still has a lot of variables, we need to use a feature selection technique to reduce the features before using a machine learning method. We chose to use model agnostic methods, because the feature selection should be valid for all methods that are later compared. In an earlier step the variables most correlated with the target variable were already identified. Unfortunately this captures only linear monotonous relationships in the data and does not work well for our nominal categorical features. 

[maybe delete chi^2 test + text]
We will also use a Chi^2 test between our variables and our target variable to assess their relationship with regards to independence. The variables that are found to have a significant relationship (p >= 0.05 %) with the target are kept. 

To capture non-linear relationships as well, information gain between the target and the predictor variables is measured as well. The variables with high information gain with respect to the target variable are kept, because they can contribute more in predicting the target variable.

#### 3.1.3 Feature Selection with Chi^2

```{r chi^2 feature selection, include=FALSE}
# data
data<-RISK_2000_5
# Target variable
target_variable <- RISK_2000_5$suicidal_class

# Initialize an empty data frame to store chi-squared results
chi_squared_results <- data.frame(Variable = character(), Chi_squared = numeric(), P_value = numeric(), stringsAsFactors = FALSE)

# Loop through all variables
for (var in names(data)) {
    if (var != "suicidal_class") { 
        # Compute the chi-squared test
        test_result <- chisq.test(table(data[[var]], target_variable))
        
        # Store the results
        chi_squared_results <- rbind(chi_squared_results, data.frame(Variable = var, Chi_squared = test_result$statistic, P_value = test_result$p.value))
    }
}

# Sort by p-value to find the most significant variables
significant_vars <- chi_squared_results %>% arrange(P_value) %>% filter(P_value < 0.05) # You can adjust the threshold as needed

# Print the significant variables
print(significant_vars)
rm(target_variable, var, data, chi_squared_results, test_result)
```


#### 3.1.4 Information Gain for Feature Selection


```{r mutual information, include=FALSE}

# compute information gain
scores <- information.gain(suicidal_class ~ ., RISK_2000_5)

# sort the features regarding their information gain
sorted_scores <- scores[order(-scores$attr_importance), ]

# select top 20 features
top_20_features <- head(sorted_scores, 20)

# Get the names of all features (excluding the target variable 'suicidal_class')
feature_names <- setdiff(names(RISK_2000_5), "suicidal_class")

# Match the sorted scores with the correct feature names
top_20_feature_names <- feature_names[order(-scores$attr_importance)[1:20]]

# Create a named vector for the top 20 features and their scores
named_top_20_features <- setNames(top_20_features , top_20_feature_names)

print(named_top_20_features)
rm(scores, sorted_scores, top_20_feature_names, top_20_features, feature_names)
```

#### 3.1.5 Domain Knowledge for Final Feature Selection

```{r difference between correlation and information gain feature selection, echo=FALSE}
setdiff(names(named_top_20_features), corr_vars$var)
```
\

Let's see what those variables actually stand for. "q7orig" and "q6orig" cannot be found in the data manual and will therefore be discarded. According to the data manual, "PSUs consist of counties, groups of smaller adjacent counties, or sub-areas of very large counties. “PSU” indicates the PSU the school the student attends was assigned to." (p.14). It is possible, that the district/locality of a school plays a role in the risk of suicide. For example for queer students in a very religious place. Q22 is the variable that describes physical dating violence. Therefore q22 is also a valid choice as a predictor variable for our suicidal score target variable. Q40 encodes the range of age when a student first got into contact with drinking alcohol. This might be an indicator for a negligent social surrounding if someone is exposed to an alcoholic drink in an early age and therefore also could be a valid predictor variable in our case. 

```{r setup final reduced dataframe, include=FALSE}
# set up final variables
final_vars<-c("psu", "q40", "q22", "suicidal_class", intersect(names(named_top_20_features), corr_vars$var))
print(final_vars)

# all variables highly correlated with target variable
RISK_red<-RISK_reduced[,final_vars] 

```




#### 3.2 Naming the Variables 
\

For an easy understanding of their values, the variables and levels are named according to their content.
This is an optional step. It can lead to a better readability of our data frame though.

```{r naming variables, include=FALSE}
source("names.R") 
rm(significant_vars, corr_vars, named_top_20_features, RISK_reduced)
```



#### 3.3 Splitting the Data
\

According to the project requirements we split our data in 60% Training, 20% Validation and 20% Testing Data. 
Since we want to do a cross validation we will split into 80% Training and 20% Testing Data.

```{r splitting the data, include=FALSE}
# import as dataframe
# RISK_red <- read_csv("RISK_red.csv", show_col_types = FALSE)

# https://cran.r-project.org/web/packages/splitTools/vignettes/splitTools.html
inds <- partition(RISK_red$suicidal_class, p = c(train = 0.8, test = 0.2))

# since we cross-validate for HPO and RFE, we will combine the test and val dataset
RISK_train <- RISK_red[inds$train, ]

RISK_test <- RISK_red[inds$test, ]

rm(inds, RISK_2000_5, variable_types)
```



### 4. Machine Learning Models
\
In the following, we compare two Supervised Learning methods on our reduced YOUTH AT RISK dataset to predict our suicidal_class target variable. Because we want to be able to also capture non-linear relationships in our data, we chose Support Vector Machines and Naive Bayes.

#### 4.1 Short Mathematical Overview on the used Methods
\


#### 4.1.1 Naive Bayes Classification
\ 

Naive Bayes Classifiers differs from a theoretically ideal Bayes Classification by assuming the independence of predictor variables. 



#### 4.1.2 Support Vector Machines (SVM)
\ 

The name Support Vector Machines already describes some elements of this method. A certain number of data points will define the (linear) boundary between two classification regions, these are called the support vectors. The support vectors are the datapoints (observations) that lie closest to our decision boundary. The boundary in two dimensional space is a line, in three dimensions a plane and in more than three dimensional space a hyperplane. For our dataset, we need a multidimensional hyperplane. The number of dimensions depend on the number of our predictor variables. We need to find the hyperplane, that separate our data into the classes of our target variable best. The best hyperplane is the one that maximizes the margin between the support vectors of the different classes. The margin is a strip on each of the boundaries sides. In the case of a hard classifier this strip does not contain any points. But we will have a soft classifier with the cost C as a hyperparameter. This cost C describes a budget that we allow for points within the margin or on the other side of the boundary. Depending on the position of the point, the amount it attributes to the total cost changes. A point on the correct side of the margin will not attribute to the total cost at all. If it is in the margin, but on the correct side of the boundary it will attribute between zero and one, if on the wrong side of the boundary but within the margin it will attribute with one to two and if on the wrong side of the margin it will cost more than two.
Mathematically we need to maximize the Margin M with respect to $\alpha_{0}, \alpha_{i}$ and $\epsilon_{i}$ in the following objective function:
$$y_{i}\left(\alpha_{0}+\sum_{j=1}^{n} \alpha_{i} K(\mathbf x_{i}, \mathbf x_{j})\right ) \ge M(1-\epsilon_{i})$$  The following constraints are given: $\sum_{i=1}^{n} \alpha_{i}^{2}=1$ , $\epsilon_{i} \ge 0$ and $\sum_{i=1}^{n} \epsilon_{i} < C$ with C $\ge 0$ and $i=1,...,n$
\

For our SVM approach, we will try different kernels K as hyperparameters:

\
A linear Kernel $$ K(u,v) = \langle \mathbf u, \mathbf v \rangle = \sum_{j=1}^{p}u_{j}v_{j}$$
a polynomial Kernel $$ K(u,v) = (c + \langle \mathbf u, \mathbf v \rangle)^{d}, \ with \ d > 1$$
\
and a radial Kernel $$ K(u,v)= \exp \left(-\gamma \sum_{i=1}^{p}(u_{i}-v_{i} \right)$$

#### 4.2 Preprocessing for SVM 
\

svm in R tutorial + nice visualization:
https://www.datacamp.com/tutorial/support-vector-machines-r



```{r scaling and normalizing, include=FALSE}
# scaling / normalizing the data for svm

# scale features that are numeric
numeric_cols <- sapply(RISK_train, is.numeric)
# scale only numeric columns
scaled_num_cols <- RISK_train[, numeric_cols]
scaled_num_cols <- as.data.frame(scale(scaled_num_cols))

# Combine scaled features with non-numeric data
other_cols <- RISK_train[, !numeric_cols]
scaledRISK_train <- cbind(scaled_num_cols, other_cols)

```



#### 4.3 SVM Fitting and Hyperparameter Optimization
\

for SVM:
try different available kernels and other hyperparameters
and cross validate 

```{r svm, include=FALSE, eval= FALSE}
# This is already HPO 
svm_grid <- expand.grid(sigma = c(0.001, 0.01, 0.1),
                       C = c(1, 10, 100))
                       #gamma = c(0.01, 0.1, 1),
                       #kernel = c("radial", "linear", "polynomial"),   #kernels
                       #degree = c(2, 3))  # of the polynomial kernel

# linear kernel
C <- c(1, 10, 100)

grid_linear <- expand.grid(.C = C)

# radial basis function kernel
grid_radial <- expand.grid(.C = C, 
                           .sigma = c(0.001, 0.01, 0.1))

# polynomial kernel
grid_poly <- expand.grid(.degree = c(2, 3, 4),
                         .scale = c(0.1, 1, 10),
                         .C = C)


train_control <- trainControl(method = "cv", number = 5, search = "grid")
          
train_svm <- function(kernel, grid) {
    cat(paste("Training with", kernel, "kernel started using", grid, "...\n"))

    # Train the model
    train(suicidal_class ~ ., data = scaledRISK_train,
                   method = kernel,
                   tuneGrid = grid, 
                   trControl = train_control)
    cat(paste("Training with kernel", kernel, "done.\n"))
}
# Train models for each kernel
model_linear <- train_svm("svmLinear", grid_linear)
model_radial <- train_svm("svmRadial", grid_radial)
model_poly <- train_svm("svmPoly", grid_poly)

# Output the best models and their performance
models <- list(linear = model_linear, radial = model_radial, polynomial = model_poly)

```

Given the list of all resulting models, coming from the HPO of the SVM, we extract the best model for further inspection and results.

```{r evaluation, include=FALSE, eval=FALSE}
# Function to extract the best accuracy from each model
extract_best_accuracy <- function(model) {
    max_accuracy <- max(model$results$Accuracy)
    return(max_accuracy)
}

# Apply the function to each model and collect accuracies
model_accuracies <- sapply(models, extract_best_accuracy)

# Identify the model with the highest accuracy
best_model_name <- names(which.max(model_accuracies))
best_model <- models[[best_model_name]]

# Extract the best hyperparameters
best_hyperparameters <- best_model$bestTune

# Print the best hyperparameters
cat(paste("Best hyperparameters for the model with", best_model_name, "kernel and HP", colnames(best_hyperparameters), "=", best_hyperparameters))


# Print the name and details of the best model
print(paste("Best performing model is:", best_model_name))
print(best_model)

# Print the best model's name and its details
print(paste("Best model is:", best_model_name))
print(best_model)
model <- best_model
```



```{r print model with stargazer, echo=FALSE}

# stargazer(model, type = "text") # Modell ausgeben mit stargazer
```


\


```{r Signifikanz der Modelle, include= FALSE}
# lrtest(model_best) # erarbeitetes Modell ist besser als Nullmodell?
```

Mithilfe eines Likelihood-Ratio Tests wird überprüft welches Modell das bessere ist.
```{r Vergleich der Modelle, echo=FALSE}

# lrtest(model_best, model) # restringiertes Modell und optimiertes Modell mit mehr IA-Termen

```
Das optimierte Modell ist signifikant besser als das erarbeitete zum festgelegten Signifikanzniveau,
die Nullhypothese des LR-Tests, dass das optimierte Modell nicht besser ist als das komplexere, kann verworfen werden. Wir stellen außerdem fest: Beide Modelle weisen einen signifikanten Erklärungsgehalt auf, sie sind besser als das Nullmodell.

### 5. Comparison of the Models / Model's Performance on Test Data
\
if one model performs better, is this improvement significant to a usual significance level?

#### 5.1 Quantitative 

##### 5.1.1 Confusion Matrix

##### 5.1.2 Accuracy

##### 5.1.2 Precision, Recall and F1-Score

#### 5.2 Qualitative


#### 5.3 Overfitting Check
\
compare training/validation and testing curves...

### 6. Visual Representation
\
Dimensionality Reduction Techniques: Sometimes, it's helpful to use dimensionality reduction techniques (like PCA) to identify the most significant variables or components and then focus the GAM analysis and visualization on these.



### 7. Final Discussion



### 8. References

> <a id="id_1">[1]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm>

> <a id="id_2">[2]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2021/2021_YRBS_Data_Users_Guide_508.pdf>