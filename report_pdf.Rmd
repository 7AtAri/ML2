---
title: "Projectreport Machine Learning 2"
header-includes:
  - \usepackage[english]{babel}
  - \DeclareUnicodeCharacter{308}{oe}  
author: "Maluna Menke, Ari (Sara) Wahl, Pavlo Kravets"
date: "`r Sys.Date()`"

output:
  #html_document: 
  pdf_document:
    
    toc: true
    toc_depth: 6
  start-page: 2
---

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic} 
\centering
\raggedright
\newpage

---

```{r setup, include=FALSE, cache=TRUE}
params <- list(html=knitr::is_html_output())
knitr::opts_chunk$set(echo = params$html)
rm(list = ls(all.names = TRUE)) # reset the environment
set.seed(0)  # set the seed
#names(RISK)

source("Setup.R")
```

### 1. Introduction

Our general idea was to work with LGBT-related data. This was not as easy as expected, since it seems there are not a lot of datasets openly available that have that kind of information. Finally, we found a US survey by the CDC, that regularly monitors the country's youth in a lot of dimensions, but among other questions also asks for sexual experiences and identification.   

### 2. The Dataset
\
"The *[Youth Risk Behavior Survey (YRBS)](https://www.cdc.gov/healthyyouth/data/yrbs/data.htm)* measures health-related behaviors and experiences that can lead to death and disability among youth and adults.[...] Some of the health-related behaviors and experiences monitored are:
- Student demographics: sex, sexual identity, race and ethnicity, and grade
- Youth health behaviors and conditions: sexual, injury and violence, bullying, diet and physical activity, obesity, and mental health, including suicide
- Substance use behaviors: electronic vapor product and tobacco product use, alcohol use, and other drug use
- Student experiences: parental monitoring, school connectedness, unstable housing, and exposure to community violence [[1]](#1).
It is a national survey conducted by the CDC (Center for Disease Control and Prevention) and includes high school students from both private and public schools within the U.S. Data is collected from 1991 through 2021, we are only using the most recent data from 2021. If you want to learn more about the data there is an accompanying Data User Guide.[[2]](#2).


#### 2.1 Preprocessing of the dataset
\

```{r summary, include=FALSE, cache=TRUE}
# possible summaries of the data:
# summary(RISK) 
# describe(RISK) # more detail
```

To preprocess the dataset, we first ran a summary of our dataset. The number of NAs seems to depend very much on the question. The variable "orig_rec" only contained NAs and has therefore been removed, as well as the variable "site" which only contained "XX" entries. Variables q4 and q5 are already aggregated in "raceeth" and have also been deleted.
The variable "record" seems to be an ID for the observations. This has to be considered later. 

#### 2.2 Missing Data

```{r variables overview, include =FALSE}
na_ratio<-sum(is.na(RISK)==TRUE)/(dim(RISK)[1]*dim(RISK)[2])# ratio of NAs among all datapoints
print(na_ratio*100) # % percentage of NAs in data
sum(is.na(RISK)==TRUE) # sum of NAs in dataset
```
\
We will first exclude all the observations with NAs in all the target-related variables q25 to q29. Since we want to build our target variable on these questions, the target variable cannot all be empty. The amount of data available should be enough to just exclude these observations. After removing the observations that have NAs in all the variables, that are used to create our target variable, we still have around 13.7% NAs in the dataset.
\
What if we had just excluded every NA in the dataset? We will try and see if this is a viable option, since this woul not just be quick and easy, but we would also just have "real" answers. The exclusion of NAs leads to a severe reduction in the number of observations. The original data consisted of 17232 observations, after reducing the target-related NAs only, we have 11753 observations left. If we omit all NAs, the reduced dataframe still has 4334 observations.
\

```{r check exclude NAs, include=FALSE, cache=TRUE}
RISK_no_NAs <- na.omit(RISK)
summary(RISK_no_NAs)
```

In this case need to assess the loss of information foremost about our target variable. The important question is if there is a pattern to the missingness in our data, not just, but especially about our target variable.

```{r Tables for Missingness in q28, include=FALSE, cache=TRUE}
table(RISK_no_NAs$q28)
table(RISK$q28)

table(RISK_no_NAs$q28)/4334
table(RISK$q28)/17138
sum(is.na(RISK$q28)==TRUE)/17138
```

#### 2.2.2 Omitting NAs vs Data Imputation
\
If we can omit the NAs or if it may be necessary to impute the missing data points, depends on the type of missingness. If data is missing completely at random (MCAR), we can omit the NAs, if it is just missing at random (MAR) we would rather impute the data. If the data is missing not at random (MNAR), it would be a quite difficult problem because we cannot easily impute the missing data then. To find out if we can just omit the data, an MCAR test was applied. 
\
We test the target-related variables q25 to q29 for potential pattern(s) in the missing data. This results in a p-value of 0, which means we can say for sure, that the data is not missing completely at random. Just omitting all NAs could be problematic and lead to bias.
\

```{r MCAR test for target columns, cache=TRUE}
result <- mcar_test(RISK[, c("q25", "q26", "q27", "q28", "q29")])
print(result)
```
Because of this, we will use a rule base approach to create the target variable and impute the predictive variables afterwards. To ensure a good imputation, we need to impute our NAs before reducing the dataset to 2000 observations. To run the imputation properly we need to factorize our nominal and ordinal variables first.
```{r analyse missingness, include=FALSE, cache=TRUE}
plot_missing(RISK[1:20])
plot_missing(RISK[20:40])
plot_missing(RISK[40:60])
plot_missing(RISK[60:80])
plot_missing(RISK[80:100])
plot_missing(RISK)
```


#### 2.3 Target Variable
\
As a target variable, we decided to calculate a score from 5 questions that reflect the suicide risk of the person (observation) in question. This score is aggregated with a rule-based approach. 

```{r creating the target variable, include=FALSE, cache=TRUE}
RISK_num<-RISK
RISK_num$q25 <- as.numeric(RISK_num$q25)
RISK_num$q26 <- as.numeric(RISK_num$q26)
RISK_num$q27 <- as.numeric(RISK_num$q27)
RISK_num$q28 <- as.numeric(RISK_num$q28)
RISK_num$q29 <- as.numeric(RISK_num$q29)

# TODO: think about reducing to 3 classes 
RISK_s3 <- RISK_num
RISK_s3$suicidal_class <- ifelse(RISK_s3$q29 > 1, 3,
                          ifelse(RISK_s3$q28 > 1, 3, 
                          ifelse(RISK_s3$q27 == 1, 3,
                          ifelse(RISK_s3$q26 == 1, 2,
                          ifelse(RISK_s3$q25 == 1, 2, 1)))))

RISK_s5 <- RISK_s3 %>% 
  mutate(suicidal_class = ifelse(q29 > 1,   5, # 3
                          ifelse(q28 > 1,   5, # 3
                          ifelse(q27 == 1,  4, # 3
                          ifelse(q26 == 1,  3, # 2
                          ifelse(q25 == 1,  2,
                                            1))))))

RISK_s3[, c("q25", "q26", "q27", "q28", "q29")] <- NULL
RISK_s5[, c("q25", "q26", "q27", "q28", "q29")] <- NULL

RISK_s3 <- RISK_s3[!is.na(RISK_s3$suicidal_class), ]
RISK_s5 <- RISK_s5[!is.na(RISK_s5$suicidal_class), ]
# removing the columns related to the target variable

RISK_s3$suicidal_class<-factor(RISK_s3$suicidal_class, ordered = TRUE)
RISK_s5$suicidal_class<-factor(RISK_s3$suicidal_class, ordered = TRUE)

#factorize the target variable
```

\ 
After creating the target variable we need to exclude the variables q25 to q29, which were used for creating it, from our dataset. 

#### 2.4 Imputation


```{r imputing, include= FALSE,cache=TRUE}
set.seed(0)
# the categorical variables are factorized in the setup.R file
RISK_imp_s3 <- missRanger(RISK_s3, pmm.k = 200, num.trees = 500, sample.fraction = 1)
write_csv(RISK_imp_s3, "RISK_imp_s3.csv")
```


#### 2.5 Reducing and balancing the dataset to 2000 observations
\
We need to reduce our data to the maximum allowed size of 2000 observations. To ensure the best possible data quality, we want to ensure that our dataset is balanced. Intuitively, we are considering if it is best to still use as much of the non-imputed data for our smaller dataset as possible, before filling it up with imputed data, since non-imputed data is usually of better quality. On the other hand the data seemingly shows patterns in the missingness so there are reasons to just do a stratified sampling over the imputed data as well. To do a proper statified sampling we need to identify the stratification variables. Therefore we will calculate the correlations with the target variable and see which variables are highly correlated to our target variable. These will then as well as the target variable be used as stratification variables.

```{r stratified sampling, include=FALSE, cache=TRUE}
# this code snipped was produced with the help of AI
# *

set.seed(0)  # set the seed
RISK_imp_s3 <- read_csv("RISK_imp_s3.csv")
size_per_class <- rep(666, times = 3) # 400 samples from each of 5 classes, adjust as needed
sample_obj <- strata(RISK_imp_s3, stratanames = "suicidal_class", size = size_per_class, method = "srswor")
RISK_2000_3 <- getdata(RISK_imp_s3, sample_obj)
write_csv(RISK_2000_3, "RISK_2000_3.csv")

# *
# clean up:
# rm(RISK, RISK_imp_num, RISK_imp, sample_obj, strat_df)  # remove unnecessary variables in the environment

```



#### 2.6 Simple Synopsis of the Dataset



```{r RISK_2000_5 load and inspect, include=FALSE, cache=TRUE}
source("Setup_RISK.R")

variable_types <- skim(RISK_2000_3) # get the variable types
summary(variable_types) # summary on variable types
# run lines below separately again to show: (command+enter)
table(variable_types$factor.ordered) # summary on factor variables
variable_types[variable_types$skim_type=="numeric",]
# introduce(RISK_2000_3)
```
\
number of observations: 2000 
number of variables: 100

datatypes:
factor: 96
- nominal variables: 29
- ordinal variables: 67
numeric variables: 4
- discrete variables: 96 (here all factor variables)
- continuous variables: 4 (here all numeric variables)


### 3. Additional Data Preparation

question: would it introduce information leakage to reduce the features before splitting the data?


```{r check for constant or near zero variance features and eliminate them, include=FALSE, cache=TRUE}

# ** this codeblock was done using AI **
# check if there are constant features
constantFeatures <- sapply(RISK_2000_3, function(x) {
    if(is.factor(x) || is.character(x)) {
        return(length(unique(x)) == 1)
    } else {
        return(var(x, na.rm = TRUE) == 0)
    }
})

constantFeatureNames <- names(RISK_2000_3)[constantFeatures]

# Print constant features
print(constantFeatureNames)

# Check for near-zero variance features
nzvFeatures <- nearZeroVar(RISK_2000_3, saveMetrics = TRUE)
nzvFeatureNames <- names(RISK_2000_3)[nzvFeatures$nzv]
if(any(nzvFeatures$nzv)) {
    print("Near-zero variance features found:")
    print(names(RISK_2000_3)[nzvFeatures$nzv])
} else {
    print("No near-zero variance features.")
}

# Remove near-zero variance features
RISK_reduced <- RISK_2000_3[, !names(RISK_2000_3) %in% nzvFeatureNames]

# ** end of AI produced code block **
rm(nzvFeatures, constantFeatureNames, constantFeatures, nzvFeatureNames)
write_csv(RISK_reduced,"RISK_2000_3s_red.csv")
```


#### 3.1 Feature Reduction
\
Since our dataset has lots of variables, we decided to start by excluding some variables depending on the estimated feature importance.

#### 3.1.1 Correlations 
\ 
Unfortunately at this point we have to many variables to do a pairs plot or correlation plot with a visually usable outcome. We will therefore perform a correlation analysis only with respect to the target variable and in numeric format instead of any visual plot.

```{r, corr plot,echo=FALSE, cache=TRUE}
#TODO:
RISK_reduced <-read_csv("RISK_2000_3s_red.csv")
# plot_correlation(RISK_2000_3)
#pairs(RISK) # -> non numeric arguments included
#plot(RISK) 

# corr_mat=cor(RISK, method="s") #create spearman correlation matrix

#corrplot(corr_mat, method = "color",
#     type = "upper", order = "hclust", 
#     tl.col = "black") 

# preprocessing for correlations:
RISK_num <- as.data.frame(lapply(RISK_reduced, as.numeric)) # convert all variables to numeric
corr_mat=cor(RISK_num, method="s") # correlation matrix
target_corr <- corr_mat[, "suicidal_class"] # get correlations with respect to the target variable
# df with variable names and their correlation values:
corr_data <- data.frame(
  var = names(target_corr),
  corr = target_corr
)
 
rm(RISK_num, target_corr, corr_mat) # not needed any more

# Sort the data frame by correlation values
sorted_corr_data <- corr_data[order(-corr_data$corr), ]

# stratification variable vector:
corr_vars <- sorted_corr_data[abs(sorted_corr_data$corr) > 0.25, ]
# select all correlations as possible strata (pos & neg) bigger than 0.25
corr_vars <- corr_vars$var[!is.na(corr_vars$var)]

# all variables highly correlated with target variable
RISK_red<-RISK_reduced[, corr_vars] 
#TODO: remove prob?
RISK_red$Stratum <- NULL
RISK_red$Prob <- NULL
# correlations of these
plot_correlation(RISK_red)
rm(sorted_corr_data, corr_data)
```


```{r, cache=TRUE}
print(corr_vars)
```

The 18 variables with high correlations (>0.25) with our target variable are:
q93                 
q85                     
q45           
q35          
q36               
q39                  
q64                 
q46               
q20                   
q30                     
q24                     
q19                       
q98                       
q34 
q41
q43
q44
q47



#### 3.1.2 Feature Selection Algorithm

Since the data still has a lot of variables, we need to use a feature selection technique to reduce the features before using a machine learning method. We chose to use model agnostic methods, because the feature selection should be valid for all methods that are later compared. In an earlier step the variables most correlated with the target variable were already identified. Unfortunately this captures only linear monotonous relationships in the data and does not work well for our nominal categorical features. 

[maybe delete chi^2 test + text]
We will also use a Chi^2 test between our variables and our target variable to assess their relationship with regards to independence. The variables that are found to have a significant relationship (p >= 0.05 %) with the target are kept. 

To capture non-linear relationships as well, information gain between the target and the predictor variables is measured as well. The variables with high information gain with respect to the target variable are kept, because they can contribute more in predicting the target variable.

#### 3.1.3 Feature selection with chi^2

```{r chi^2 feature selection, include=FALSE, cache=TRUE}
# data
data<-RISK_2000_3
# Target variable
target_variable <- RISK_2000_3$suicidal_class

# Initialize an empty data frame to store chi-squared results
chi_squared_results <- data.frame(Variable = character(), Chi_squared = numeric(), P_value = numeric(), stringsAsFactors = FALSE)

# TODO:
# Warning: Chi-squared approximation may be incorrect

# Loop through all variables
for (var in names(data)) {
    if (var != "suicidal_class") { 
        # Compute the chi-squared test
        test_result <- chisq.test(table(data[[var]], target_variable))
        
        # Store the results
        chi_squared_results <- rbind(chi_squared_results, data.frame(Variable = var, Chi_squared = test_result$statistic, P_value = test_result$p.value))
    }
}

# Sort by p-value to find the most significant variables
significant_vars <- chi_squared_results %>% arrange(P_value) %>% filter(P_value < 0.05) # You can adjust the threshold as needed

# Print the significant variables
print(significant_vars)

rm(target_variable, var, data, chi_squared_results, test_result)
```


#### 3.1.4 Information gain for feature selection


```{r mutual information, include=FALSE, cache=TRUE}

set.seed(0)  # set the seed

# compute information gain
scores <- information.gain(suicidal_class ~ ., RISK_2000_3)

# sort the features regarding their information gain
sorted_scores <- scores[order(-scores$attr_importance), ]

# select top 20 features
top_20_features <- head(sorted_scores, 20)

# Get the names of all features (excluding the target variable 'suicidal_class')
feature_names <- setdiff(names(RISK_2000_3), "suicidal_class")

# Match the sorted scores with the correct feature names
top_20_feature_names <- feature_names[order(-scores$attr_importance)[1:20]]

# Create a named vector for the top 20 features and their scores
named_top_20_features <- setNames(top_20_features , top_20_feature_names)

print(named_top_20_features)
rm(scores, sorted_scores, top_20_feature_names, top_20_features, feature_names)
```

#### 3.1.5 Domain knowledge for final feature selection

```{r difference between correlation and information gain feature selection,echo=FALSE, cache=TRUE}
setdiff(names(named_top_20_features), corr_vars)

#TODO: q6orig not in there anymore
```

Let's see what those variables actually stand for. "q7orig" and "q6orig" cannot be found in the data manual and will therefore be discarded. According to the data manual, "PSUs consist of counties, groups of smaller adjacent counties, or sub-areas of very large counties. “PSU” indicates the PSU the school the student attends was assigned to." (p.14). It is possible, that the district/locality of a school plays a role in the risk of suicide. For example for queer students in a very religious place. Q22 is the variable that describes physical dating violence. Therefore q22 is also a valid choice as a predictor variable for our suicidal score target variable. Q40 encodes the range of age when a student first got into contact with drinking alcohol. This might be an indicator for a negligent social surrounding if someone is exposed to an alcoholic drink in an early age and therefore also could be a valid predictor variable in our case. 

```{r setup final reduced dataframe, include=FALSE, cache=TRUE}

# set up final variables
final_vars<-c("psu", "q40", "q22", "suicidal_class", intersect(names(named_top_20_features), corr_vars))
final_vars$Stratum <- NULL
final_vars$Prob <- NULL
print(final_vars)

# all variables highly correlated with target variable
RISK_red<-RISK_reduced[,unlist(final_vars)] 

```




#### 3.2 Naming the variables and the factor levels
\
For an easy understanding of their values, the variables and levels are named according to their content.
This is an optional step. It can lead to a better readability of our data frame though.

```{r naming variables, include=FALSE, cache=TRUE}
source("names.R") 
rm(significant_vars, corr_vars, named_top_20_features, RISK_reduced)
write_csv(RISK_red, "RISK_red.csv")
```



#### 3.3 Splitting the Data
\
According to the project requirements we split our data in 60% Training, 20% Validation and 20% Testing Data. 
Since we want to do a cross validation we will split into 80% Training and 20% Testing Data.

```{r splitting the data, include=FALSE, cache=TRUE}
# import as dataframe
# RISK_red <- read_csv("RISK_red.csv", show_col_types = FALSE)
set.seed(0)  # set the seed
RISK_red <- read_csv("RISK_red.csv", show_col_types = FALSE)

# https://cran.r-project.org/web/packages/splitTools/vignettes/splitTools.html
inds <- partition(RISK_red$suicidal_class, p = c(train = 0.8, test = 0.2))

# since we cross-validate for HPO and RFE, we will combine the test and val dataset
RISK_train <- RISK_red[inds$train, ]

RISK_test <- RISK_red[inds$test, ]
```



### 4. Machine Learning Models


-> not linearly separable
-> chose models that are good in separating non-linear relationships
-> chose model preferably from ML2 (at least 1)
-> chose a model preferable with results that can be visualized 

SVM and Naive Bayes


#### 4.1 Short Mathematical Overview on the used Methods
\

#### 4.1.1 Naive Bayes Classification
\ 

Ideally a classifier is able to detect the class k which maximizes the conditional probability $P(Y=k|X=x_{1},...,x_{p})$. A Bayes Classifier would calculate these probabilities for each of the classes exactly, but usually it is only possible to approximate those. The Naive Bayes Classifier is one method of approximation. It  approximates by "naively" assuming the conditional independence of predictor variables. This leads to simpler calculations. The joint probability of two events A and B $P(A \cap B)= P(A \mid B) * P(B) $ can be simplified to $P(A \cap B)= P(A) * P(B) $under the independence assumption, since conditional independence means $P(A \mid B) = P(A)$. The conditional probabilities of a class k can be calculated with the Bayes Theorem. It states that: $$P(k \mid X) = \frac {P(X\mid k) \cdot P(k)}{P(X)}$$
Since the denominator only uses our predictor variables, we only need to focus on the nominator and find the maximum class for each observation to be classified. We can express this relationship with the proportionality operator:
$$P(k|X) \propto P(X|k) \cdot P(k)$$
At this point, the assumption of independent predictor variables simplifies the calculations if there is more than one predictor variable. Instead of calculating $P(k|x_{1},...,x_{p}) \propto P(x_{1},...,x_{p}|k) \cdot P(k)$ where $P(x_{1},...,x_{p}|k)$ is quite complicated to calculate because of all the possible dependencies among the variables, the independence assumption leads to: $$P(k|x_{1},...,x_{p}) \propto P(k) \cdot  \prod_{i=1}^{p} P(x_{i}|k) $$

Often this yields good results even if the quite strong assumption of conditional independence is not met. If the dependencies do not contribute that much to the outcome, the approximation is still quite good.

#### 4.1.2 Support Vector Machines (SVM)
\ 

The name Support Vector Machines already describes some elements of this method. A certain number of data points will define the (linear) boundary between two classification regions, these are called the support vectors. The support vectors are the datapoints (observations) that lie closest to our decision boundary. The boundary in two dimensional space is a line, in three dimensions a plane and in more than three dimensional space a hyperplane. For our dataset, we need a multidimensional hyperplane. The number of dimensions depend on the number of our predictor variables. We need to find the hyperplane, that separate our data into the classes of our target variable best. The best hyperplane is the one that maximizes the margin between the support vectors of the different classes. The margin is a strip on each of the boundaries sides. In the case of a hard classifier this strip does not contain any points. But we will have a soft classifier with the cost C as a hyperparameter. This cost C describes a budget that we allow for points within the margin or on the other side of the boundary. Depending on the position of the point, the amount it attributes to the total cost changes. A point on the correct side of the margin will not attribute to the total cost at all. If it is in the margin, but on the correct side of the boundary it will attribute between zero and one, if on the wrong side of the boundary but within the margin it will attribute with one to two and if on the wrong side of the margin it will cost more than two.
Mathematically we need to maximize the Margin M with respect to $\alpha_{0}, \alpha_{i}$ and $\epsilon_{i}$ in the following objective function:
$$y_{i}\left(\alpha_{0}+\sum_{j=1}^{n} \alpha_{i} K(\mathbf x_{i}, \mathbf x_{j})\right ) \ge M(1-\epsilon_{i})$$  The following constraints are given: $\sum_{i=1}^{n} \alpha_{i}^{2}=1$ , $\epsilon_{i} \ge 0$ and $\sum_{i=1}^{n} \epsilon_{i} < C$ with C $\ge 0$ and $i=1,...,n$
\

For our SVM approach, we will try different kernels K as hyperparameters:

\
A linear Kernel $$ K(u,v) = \langle \mathbf u, \mathbf v \rangle = \sum_{j=1}^{p}u_{j}v_{j}$$
a polynomial Kernel $$ K(u,v) = (c + \langle \mathbf u, \mathbf v \rangle)^{d}, \ with \ d > 1$$
\
and a radial Kernel $$ K(u,v)= \exp \left(-\gamma \sum_{i=1}^{p}(u_{i}-v_{i} \right)$$



#### 4.2 Preprocessing

svm in R tutorial + nice visualization:
https://www.datacamp.com/tutorial/support-vector-machines-r


We scale the data as part of pre-processing. Scaling transforms the data to have unit variance, further contributing to uniformity across different scales and improving algorithm performance.

```{r scaling and normalizing, include=FALSE, cache=TRUE}

set.seed(0)  # set the seed to shuffle
# scaling / normalizing the data for svm

scaleDataset <- function(dataset){
  # scale features that are numeric
  numeric_cols <- sapply(dataset, is.numeric)
  numeric_cols["suicidal_class"] <- FALSE
  # scale only numeric columns
  scaled_num_cols <- dataset[, numeric_cols]
  scaled_num_cols <- as.data.frame(scale(scaled_num_cols))
  
  # Combine scaled features with non-numeric data
  other_cols <- dataset[, !numeric_cols]
  scaledDataset <- cbind(scaled_num_cols, other_cols)
  return (scaledDataset)
}

RISK_train$suicidal_class <- factor(RISK_train$suicidal_class, ordered = TRUE)
RISK_test$suicidal_class <- factor(RISK_test$suicidal_class, ordered = TRUE)

RISK_train$Stratum <- NULL
RISK_train$Prob <- NULL
scaledRISK_train <- scaleDataset(RISK_train)
scaledRISK_test <- scaleDataset(RISK_test)
```



#### 4.3 Hyperparameter Optimization

Hyperparameter Optimization (HPO) enables the testing of various hyperparameter combinations to identify the optimal settings that maximize our target evaluation metric, namely the model's accuracy. The grid search method allows for the exhaustive pairing of each hyperparameter with every other, albeit at a significant computational cost. This approach, however, offers the advantage of explicitly specifying the values for testing. 

Additional to scaling we also use centering as a part of preprocessing. Centering the data ensures that each feature has a mean of zero. This is particularly useful when features are on different scales and can improve the performance by removing bias due to the scale of the features. 

##### 4.3.1 HPO of Naive Bayes

##### 4.3.2 HPO of SVM
For the SVM we examine three distinct kernel types - linear, radial, and polynomial - each characterized by unique parameters, in addition to the common cost parameter.

Kernel and the cost parameter C have a significantly impact on the model's performance and need to be tuned carefully.

Additionally to scaling we're also centering the data, which involves subtracting the mean from each feature, ensuring that each feature has a mean of zero. This is particularly useful when features are on different scales and can improve the performance of support vector machines by removing bias due to the scale of the features. 
For programming purposes we use the build-in preProcess parameter of the train function where the training data is automatically scaled and centered.

# A large value of C leads to ... being heavily penalised so there are few points in the margin/missclassified (Lecture 7, p.5)


```{r training svm, include=FALSE, cache=TRUE}

set.seed(0)  # set the seed

library(caret)
# linear kernel
C <- c(0.1, 1, 10, 100)

grid_linear <- expand.grid(.C = C)

# radial basis function kernel
grid_radial <- expand.grid(.C = C, 
                           .sigma = c(0.001, 0.01, 0.1))

# polynomial kernel
grid_poly <- expand.grid(.degree = c(2, 3, 4, 5),
                         .scale = c(0.1, 0.5, 1),
                         .C = C)


train_control <- trainControl(method = "cv", number = 5, search = "grid", )
          
train_svm <- function(dataset, kernel, grid) {
    train(suicidal_class ~ ., data = dataset,
                   method = kernel,
                   tuneGrid = grid, 
                   metric="Accuracy",
                   preProcess = c("center"),
                   trControl = train_control)
}


# Train models for each kernel
model_linear <- train_svm(scaledRISK_train,"svmLinear", grid_linear)
model_radial <- train_svm(scaledRISK_train, "svmRadial", grid_radial)
model_poly <- train_svm(scaledRISK_train, "svmPoly", grid_poly)

# Output the best models and their performance
svm_models <- list(linear = model_linear, radial = model_radial, polynomial = model_poly)

```

Given the list of all resulting models, coming from the HPO of the SVM, we extract the best model for further inspection and results using the Accuracy as our focused metric.


```{r metrics calc svm, include=FALSE, cache=TRUE}

# Function to extract the best accuracy from each model
extract_best_accuracy <- function(model) {
    max_accuracy <- max(model$results$Accuracy)
    return(max_accuracy)
}

# Apply the function to each model and collect accuracies
svm_model_accuracies <- sapply(svm_models, extract_best_accuracy)

# Identify the model with the highest accuracy
svm_best_model_name <- names(which.max(svm_model_accuracies))
svm_best_model <- svm_models[[svm_best_model_name]]
# Extract the best hyperparameters
svm_best_hyperparameters <- svm_best_model$bestTune

svm_best_model <- svm_best_model$finalModel


# Print the best hyperparameters
cat(paste("Best hyperparameters for the model with", svm_best_model_name, "kernel and HP", colnames(svm_best_hyperparameters), "=", svm_best_hyperparameters))


# Print the name and details of the best model
print(paste("Best performing model is:", svm_best_model_name))
print(svm_best_model)

saveRDS(svm_best_model, file = "svm_best_model.rda")

```


### 5. Comparison of the Models / Model's Performance on Test Data
\
if one model performs better, is this improvement significant to a usual significance level?

#### 5.1 Quantitative 

##### 5.1.1 Confusion Matrix
\
###### 5.1.1.2 Naive Bayes
```{r Confusion Matrix of Naive Bayes,echo=FALSE, cache=TRUE}
scaledRISK_test_no_target <- scaledRISK_test[,-5]
# TODO:
# nb_best_model_pred <- predict(nb_best_model, scaledRISK_test_no_target)

# table(nb_best_model_pred, scaledRISK_test$suicidal_class, dnn=c("Prediction", "Actual"))   
# confusionMatrix(table(nb_best_model_pred, scaledRISK_test$suicidal_class))
```


###### 5.1.1.2 SVM 
```{r Confusion Matrix of SVM, cache=TRUE}
svm_best_model <- readRDS("svm_best_model.rda")
svm_actual <- scaledRISK_test$suicidal_class
scaledRISK_test$suicidal_class <- NULL
scaledRISK_test$Stratum <- NULL
scaledRISK_test$Prob <- NULL
svm_best_model_pred <- predict(svm_best_model, newdata=scaledRISK_test)
table(svm_best_model_pred, svm_actual, dnn=c("Prediction", "Actual"))   
confusionMatrix(table(svm_best_model_pred, svm_actual))
```
Given the Confusion Matrix we can see a Accuracy of 1 meaning the predictions are 100% correct. 
In this context, a p-value of less than 2.2e−16 for the hypothesis that "Accuracy is greater than No Information Rate" suggests that there is extremely strong statistical evidence that the accuracy of the model is better than what would be achieved by always predicting the most frequent class. This implies that the model has predictive power beyond mere chance and is effectively learning from the features in the dataset.


##### 5.1.2 Precision, Recall and F1-Score
###### 5.1.2.1 Naive Bayes
###### 5.1.2.2 SVM 
```{r Metrics of SVM, cache=TRUE}
svm_actual <- as.numeric(as.character(svm_actual))
svm_pred <- as.numeric(as.character(svm_best_model_pred))

svm_recall <- Metrics::recall(svm_actual, svm_pred)
svm_precision <- Metrics::precision(svm_actual, svm_pred)
svm_f1 <- Metrics::f1(svm_actual, svm_pred)

cat(paste("Recall:  ", svm_recall, "\nPrecison:", svm_precision, "\nF1:      ", svm_f1))
```


#### 5.2 Qualitative

---> Ari: Do we need this section? 
I am not sure if we are supposed to do a qualitative analysis,
the task only says "an appropriate assessment of the predicted values and a fair comparison of the two methods using the test data." 
I just added the section as a proposal...

##### 5.2.2 SVM without scaling 

```{r SVM no scaling, echo=FALSE, cache=TRUE}
set.seed(0)  # set the seed

# Train models for each kernel
#model_linear <- train_svm(RISK_train,"svmLinear", grid_linear)
#model_radial <- train_svm(RISK_train, "svmRadial", grid_radial)
#model_poly <- train_svm(RISK_train, "svmPoly", grid_poly)

# Output the best models and their performance
#svm_no_sc_models <- list(linear = model_linear, radial = model_radial, polynomial = model_poly)

#svm_model_no_sc_accuracies <- sapply(svm_no_sc_models, extract_best_accuracy)

# Identify the model with the highest accuracy
#svm_no_sc_best_model_name <- names(which.max(svm_model_no_sc_accuracies))
#svm_no_sc_best_model <- svm_no_sc_models[[svm_no_sc_best_model_name]]

# Extract the best hyperparameters
#svm_no_sc_best_hyperparameters <- svm_no_sc_best_model$bestTune

# Print the name and details of the best model
#print(paste("Best performing model is:", svm_no_sc_best_model_name))
#plot(svm_no_sc_best_model)
```


#### 5.3 Overfitting 
Overfitting is a frequent problem in machine learning. It happens when a model learns the training data too well, including all its quirks and noise. As a result, its ability to generalize weakens. When the model is tested with new data, its performance often drops significantly. This is because it's overly tuned to the training data and doesn't adapt well to new, unseen data.

##### 5.3.2 SVM
```{r Train and Test curve of SVM,echo=FALSE, cache=TRUE}
#extract_performance <- function(model) {
  #  results <- model$results
 #   resampling <- model$resample
 #   list(results = results, resampling = resampling)
#}

#performance_linear <- extract_performance(model_linear)
#performance_radial <- extract_performance(model_radial)
#performance_poly <- extract_performance(model_poly)

#plot_performance <- function(performance, title) {
    #p<-ggplot(data = performance$resampling, aes(x = Iter, y = Accuracy)) +
     #   geom_line() +
     #   geom_point() +
     #   facet_wrap(~ parameter, scales = "free") +
     #   ggtitle(title) +
     #   theme_minimal()
  
    # create filename from title
   # filename <- paste0(gsub(" ", "_", title), ".png") 
    # replaces every empty space in the title with underscore and adds png format 

   # # save plot as png
   # ggsave(filename, plot = p, width = 10, height = 8, dpi = 300)

    # return ggplot object
   # return(p)
  
#}

# Plot for each model
#plot_linear<-plot_performance(performance_linear, "Linear Kernel Performance")
#plot_radial<-plot_performance(performance_radial, "Radial Kernel Performance")
#plot_polynomial<-plot_performance(performance_poly, "Polynomial Kernel Performance")
```


By employing Cross-Validation (CV) and utilizing a range of performance metrics such as recall, precision, and F1-score, we try to avoid overfitting in our model. Additionally, our experiments with different kernels reveal that the linear kernel maintains robust performance, even when compared to the more complex radial and polynomial kernels. Prior to modeling, we also took the precaution of reducing the number of features, which further diminishes the risk of overfitting. Moreover, setting the cost parameter C to a moderate level – in our case, 1, which is the default value – lessens the likelihood of overfitting. Considering these factors collectively, we are confident that our model is unlikely to overfit on our training data.

### 6. Visual Representation


### 7. Final Discussion



### 8. References

> <a id="id_1">[1]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm>

> <a id="id_2">[2]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2021/2021_YRBS_Data_Users_Guide_508.pdf>