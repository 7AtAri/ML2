---
title: "Projectreport Machine Learning 2"
header-includes:
  - \usepackage[english]{babel}
  - \DeclareUnicodeCharacter{308}{oe}  
author: "Maluna Menke, Ari (Sara) Wahl, Pavlo Kravets"
date: "`r Sys.Date()`"

output:
  #html_document: 
  pdf_document:
    
    toc: true
    toc_depth: 6
  start-page: 2
---

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic} 
\centering
\raggedright
\newpage

---

```{r setup, include=FALSE}
params <- list(html=knitr::is_html_output())
knitr::opts_chunk$set(echo = params$html)
rm(list = ls(all.names = TRUE)) # reset the environment

#names(RISK)

source("Setup.R")
```

### 1. Introduction

Our general idea was to work with LGBT-related data. This was not as easy as expected, since it seems there are not a lot of datasets openly available that have that kind of information. Finally, we found a US survey by the CDC, that regularly monitors the country's youth in a lot of dimensions, but among other questions also asks for sexual experiences and identification.   

### 2. The Dataset
\
"The *[Youth Risk Behavior Survey (YRBS)](https://www.cdc.gov/healthyyouth/data/yrbs/data.htm)* measures health-related behaviors and experiences that can lead to death and disability among youth and adults.[...] Some of the health-related behaviors and experiences monitored are:
- Student demographics: sex, sexual identity, race and ethnicity, and grade
- Youth health behaviors and conditions: sexual, injury and violence, bullying, diet and physical activity, obesity, and mental health, including suicide
- Substance use behaviors: electronic vapor product and tobacco product use, alcohol use, and other drug use
- Student experiences: parental monitoring, school connectedness, unstable housing, and exposure to community violence [[1]](#1).
It is a national survey conducted by the CDC (Center for Disease Control and Prevention) and includes high school students from both private and public schools within the U.S. Data is collected from 1991 through 2021, we are only using the most recent data from 2021. If you want to learn more about the data there is an accompanying Data User Guide.[[2]](#2).


#### 2.1 Preprocessing of the dataset
\

```{r summary, include = FALSE}
# possible summaries of the data:
# summary(RISK) 
# describe(RISK) # more detail
```

To preprocess the dataset, we first ran a summary of our dataset. The number of NAs seems to depend very much on the question. The variable "orig_rec" only contained NAs and has therefore been removed, as well as the variable "site" which only contained "XX" entries. Variables q4 and q5 are already aggregated in "raceeth" and have also been deleted.
The variable "record" seems to be an ID for the observations. This has to be considered later. 

#### 2.2 Missing Data

```{r variables overview, include =FALSE}
na_ratio<-sum(is.na(RISK)==TRUE)/(dim(RISK)[1]*dim(RISK)[2])# ratio of NAs among all datapoints
print(na_ratio*100) # % percentage of NAs in data
sum(is.na(RISK)==TRUE) # sum of NAs in dataset
```
\
We will first exclude all the observations with NAs in all the target-related variables q25 to q29. Since we want to build our target variable on these questions, the target variable cannot all be empty. The amount of data available should be enough to just exclude these observations. After removing the observations that have NAs in all the variables, that are used to create our target variable, we still have around 13.7% NAs in the dataset.
\
What if we had just excluded every NA in the dataset? We will try and see if this is a viable option, since this woul not just be quick and easy, but we would also just have "real" answers. The exclusion of NAs leads to a severe reduction in the number of observations. The original data consisted of 17232 observations, after reducing the target-related NAs only, we have 11753 observations left. If we omit all NAs, the reduced dataframe still has 4334 observations.
\

```{r check exclude NAs, include=FALSE}
RISK_no_NAs <- na.omit(RISK)
summary(RISK_no_NAs)
```

In this case need to assess the loss of information foremost about our target variable. The important question is if there is a pattern to the missingness in our data, not just, but especially about our target variable.

```{r Tables for Missingness in q28, include=FALSE}
table(RISK_no_NAs$q28)
table(RISK$q28)

table(RISK_no_NAs$q28)/4334
table(RISK$q28)/17138
sum(is.na(RISK$q28)==TRUE)/17138
```

#### 2.2.2 Omitting NAs vs Data Imputation
\
If we can omit the NAs or if it may be necessary to impute the missing data points, depends on the type of missingness. If data is missing completely at random (MCAR), we can omit the NAs, if it is just missing at random (MAR) we would rather impute the data. If the data is missing not at random (MNAR), it would be a quite difficult problem because we cannot easily impute the missing data then. To find out if we can just omit the data, an MCAR test was applied. 
\
We test the target-related variables q25 to q29 for potential pattern(s) in the missing data. This results in a p-value of 0, which means we can say for sure, that the data is not missing completely at random. Just omitting all NAs could be problematic and lead to bias.
\

```{r MCAR test for target columns}
result <- mcar_test(RISK[, c("q25", "q26", "q27", "q28", "q29")])
print(result)
```
Because of this, we will use a rule base approach to create the target variable and impute the predictive variables afterwards. To ensure a good imputation, we need to impute our NAs before reducing the dataset to 2000 observations. To run the imputation properly we need to factorize our nominal and ordinal variables first.
```{r analyse missingness, include=FALSE}
plot_missing(RISK[1:20])
plot_missing(RISK[20:40])
plot_missing(RISK[40:60])
plot_missing(RISK[60:80])
plot_missing(RISK[80:100])
plot_missing(RISK)
```


#### 2.3 Target Variable
\
As a target variable, we decided to calculate a score from 5 questions that reflect the suicide risk of the person (observation) in question. This score is aggregated with a rule-based approach. 

```{r creating the target variable, include= FALSE}
RISK_num<-RISK
RISK_num$q25 <- as.numeric(RISK_num$q25)
RISK_num$q26 <- as.numeric(RISK_num$q26)
RISK_num$q27 <- as.numeric(RISK_num$q27)
RISK_num$q28 <- as.numeric(RISK_num$q28)
RISK_num$q29 <- as.numeric(RISK_num$q29)
RISK_s5 <- RISK_num %>% 
  mutate(suicidal_class = ifelse(q29 > 1,   5, # 3
                          ifelse(q28 > 1,   5, # 3
                          ifelse(q27 == 1,  4, # 3
                          ifelse(q26 == 1,  3, # 2
                          ifelse(q25 == 1,  2,
                                            1))))))

RISK_s5[, c("q25", "q26", "q27", "q28", "q29")] <- NULL  # removing the columns related to the target variable

RISK_s5$suicidal_class<-factor(RISK_s5$suicidal_class, ordered = TRUE)  # factorize the target variable
```

\ 
After creating the target variable we need to exclude the variables q25 to q29, which were used for creating it, from our dataset. 

#### 2.4 Imputation


```{r imputing, include= FALSE}
set.seed(0)
# the categorical variables are factorized in the setup.R file
RISK_imp_s5 <- missRanger(RISK_s5, pmm.k = 200, num.trees = 500, sample.fraction = 1)
```


#### 2.5 Reducing and balancing the dataset to 2000 observations
\
We need to reduce our data to the maximum allowed size of 2000 observations. To ensure the best possible data quality, we want to ensure that our dataset is balanced. Intuitively, we are considering if it is best to still use as much of the non-imputed data for our smaller dataset as possible, before filling it up with imputed data, since non-imputed data is usually of better quality. On the other hand the data seemingly shows patterns in the missingness so there are reasons to just do a stratified sampling over the imputed data as well. To do a proper statified sampling we need to identify the stratification variables. Therefore we will calculate the correlations with the target variable and see which variables are highly correlated to our target variable. These will then as well as the target variable be used as stratification variables.

```{r stratified sampling, include = FALSE}
# this code snipped was produced with the help of AI
# *
size_per_class <- rep(400, times = 5) # 400 samples from each of 5 classes, adjust as needed
sample_obj <- strata(RISK_imp_s5, stratanames = "suicidal_class", size = size_per_class, method = "srswor")
RISK_2000_5 <- getdata(RISK_imp_s5, sample_obj)
# *
# clean up:
# rm(RISK, RISK_imp_num, RISK_imp, sample_obj, strat_df)  # remove unnecessary variables in the environment

```



#### 2.6 Simple Synopsis of the Dataset



```{r RISK_2000_5 load and inspect, include=FALSE}
source("Setup_RISK.R")

variable_types <- skim(RISK_2000_5) # get the variable types
summary(variable_types) # summary on variable types
# run lines below seperately again to show: (command+enter)
table(variable_types$factor.ordered) # summary on factor variables
variable_types[variable_types$skim_type=="numeric",]
# introduce(RISK_2000_5)
```
\
number of observations: 2000 
number of variables: 100

datatypes:
factor: 96
- nominal variables: 29
- ordinal variables: 67
numeric variables: 4
- discrete variables: 96 (here all factor variables)
- continuous variables: 4 (here all numeric variables)


### 3. Additional Data Preparation

question: would it introduce information leakage to reduce the features before splitting the data?

```{r check for constant or near zero variance features and eliminate them, include=FALSE}
# ** this codeblock was done using AI **
# check if there are constant features
constantFeatures <- sapply(RISK_2000_5, function(x) {
    if(is.factor(x) || is.character(x)) {
        return(length(unique(x)) == 1)
    } else {
        return(var(x, na.rm = TRUE) == 0)
    }
})

constantFeatureNames <- names(RISK_2000_5)[constantFeatures]

# Print constant features
print(constantFeatureNames)

# Check for near-zero variance features
nzvFeatures <- nearZeroVar(RISK_2000_5, saveMetrics = TRUE)
nzvFeatureNames <- names(RISK_2000_5)[nzvFeatures$nzv]
if(any(nzvFeatures$nzv)) {
    print("Near-zero variance features found:")
    print(names(RISK_2000_5)[nzvFeatures$nzv])
} else {
    print("No near-zero variance features.")
}

# Remove near-zero variance features
RISK_reduced <- RISK_2000_5[, !names(RISK_2000_5) %in% nzvFeatureNames]

# ** end of AI produced code block **
```


#### 3.1 Feature Reduction
\
Since our dataset has lots of variables, we decided to start by excluding some variables depending on the estimated feature importance.

#### 3.2.1 Correlations 
\ 
Unfortunately at this point we have to many variables to do a pairs plot or correlation plot with a visually usable outcome. We will therefore perform a correlation analysis only with respect to the target variable and in numeric format instead of any visual plot.

```{r, corr plot, echo=FALSE}
# plot_correlation(RISK_2000_5)
#pairs(RISK) # -> non numeric arguments included
#plot(RISK) 

# corr_mat=cor(RISK, method="s") #create Spearman correlation matrix

#corrplot(corr_mat, method = "color",
#     type = "upper", order = "hclust", 
#     tl.col = "black") 

# preprocessing for correlations:
RISK_num <- as.data.frame(lapply(RISK_reduced, as.numeric)) # convert all variables to numeric
corr_mat=cor(RISK_num, method="s") # correlation matrix
target_corr <- corr_mat[, "suicidal_class"] # get correlations with respect to the target variable
# df with variable names and their correlation values:
corr_data <- data.frame(
  var = names(target_corr),
  corr = target_corr
)
 
rm(RISK_num) # not needed any more

# Sort the data frame by correlation values
sorted_corr_data <- corr_data[order(-corr_data$corr), ]

# stratification variable vector:
corr_vars <- sorted_corr_data[abs(sorted_corr_data$corr) > 0.25,] 
# select all correlations as possible strata (pos & neg) bigger than 0.25


# all variables highly correlated with target variable
RISK_red<-RISK_reduced[, corr_vars$var] 
# correlations of these
plot_correlation(RISK_red)
```


```{r}
print(corr_vars)
```

The 18 variables with high correlations (>0.25) with our target variable are:
q93                 
q85                     
q45           
q35          
q36               
q39                  
q64                 
q46               
q20                   
q30                     
q24                     
q19                       
q98                       
q34 
q41
q43
q44
q47


#### 3.3 Splitting the Data
\
According to the project requirements we split our data in 60% Training, 20% Validation and 20% Testing Data. 


```{r splitting the data, include=FALSE}
# https://cran.r-project.org/web/packages/splitTools/vignettes/splitTools.html
inds <- partition(RISK_red$suicidal_class, p = c(train = 0.8, test = 0.2))

# since we cross-validate for HPO and RFE, we will combine the test and val dataset
RISK_train <- RISK_red[inds$train, ]

RISK_test <- RISK_red[inds$test, ]

```


#### 3.4 Feature Selection Algorithm

```{r , include=FALSE}
# http://topepo.github.io/caret/recursive-feature-elimination.html
# scaling / normalizing the data for svm

# scale features that are numeric
numeric_cols <- sapply(RISK_train, is.numeric)
# scale only numeric columns
scaled_num_cols <- RISK_train[, numeric_cols]
scaled_num_cols <- as.data.frame(scale(scaled_num_cols))

# Combine scaled features with non-numeric data
other_cols <- RISK_train[, !numeric_cols]
scaledRISK_train <- cbind(scaled_num_cols, other_cols)

rfe_control <- rfeControl(functions = caretFuncs,
                      method = "cv",
                      number = 5)
svm_grid <- expand.grid(sigma = c(0.001, 0.01, 0.1),
                       C = c(1, 10, 100))
                       #gamma = c(0.01, 0.1, 1),
                       #kernel = c("radial", "linear", "polynomial"),   #kernels
                       #degree = c(2, 3))  # of the polynomial kernel

train_control <- trainControl(method = "cv", number = 5, search = "grid")
          
set.seed(123)
svmModel <- train(suicidal_class ~ ., data = scaledRISK_train, 
                  method = "svmRadial", # svmRadial uses e1071 under the hood
                  tuneGrid = svm_grid, 
                  trControl = train_control)

```

```{r}
# feature elimination:

# Convert factors to dummy variables (one-hot encoding)
dummy_vars <- model.matrix(~ . - 1, data = scaledRISK_train[, 1:82])

# Bind the numeric variables back to the dummy variables
# numeric_vars <- scaledRISK_train[sapply(scaledRISK_train, is.numeric)]
processed_data <- as.data.frame(dummy_vars)

```



```{r}
rfe_out <- rfe(x = processed_data, 
                  y = scaledRISK_train$suicidal_class,
                  sizes = c(30,20,10),
                  rfeControl = rfe_control,
                  method = "svmRadial",  # Use svmRadial, but grid includes others
                  tuneGrid = svm_grid,
                  trControl = train_control)
print(rfe_out)
```

#### 3.3 Naming the variables and the factor levels
\
For an easy understanding of their values, the variables and levels are named according to their content.
This is an optional step. It can lead to a better readability of our data frame though.

```{r naming variables, include=FALSE}
# source("names.R") 

# todo finish the naming for all variables...
# maybe only after the feature reduction step
```






### 4. Machine Learning Models

choice of models:
PCA with visual for first 2 or 3 dimensions...
-> not linearly seperable
-> chose models that are good in seperating non-linear relationships
-> chose model preferably from ML2 (at least 1)
-> chose a model preferable with results that can be visualized 

SVM and GAMs 


#### 4.1 Short Mathematical Overview on the used Methods
\
todo


#### 4.2 Fitting process

svm in R tutorial + nice visualization:
https://www.datacamp.com/tutorial/support-vector-machines-r

```{r svm}

fit = svm(suicidal_class ~ ., data = RISK_2000_5, scale = FALSE, kernel = "radial", cost = 5)
```




#### 4.3 Hyperparameter Optimization

for SVM:
try different available kernels and other hyperparameters
and cross validate 

```{r , include=FALSE}

```


```{r print model with stargazer, echo=FALSE}

# stargazer(model, type = "text") # Modell ausgeben mit stargazer
```


\


```{r Signifikanz der Modelle, include= FALSE}
# lrtest(model_best) # erarbeitetes Modell ist besser als Nullmodell?
```

Mithilfe eines Likelihood-Ratio Tests wird überprüft welches Modell das bessere ist.
```{r Vergleich der Modelle, echo=FALSE}

# lrtest(model_best, model) # restringiertes Modell und optimiertes Modell mit mehr IA-Termen

```
Das optimierte Modell ist signifikant besser als das erarbeitete zum festgelegten Signifikanzniveau,
die Nullhypothese des LR-Tests, dass das optimierte Modell nicht besser ist als das komplexere, kann verworfen werden. Wir stellen außerdem fest: Beide Modelle weisen einen signifikanten Erklärungsgehalt auf, sie sind besser als das Nullmodell.

### 5. Comparison of the Models / Model's Performance on Test Data
\
if one model performs better, is this improvement significant to a usual significance level?

#### 5.1 Quantitative 

##### 5.1.1 Confusion Matrix

##### 5.1.2 Accuracy

##### 5.1.2 Precision, Recall and F1-Score

#### 5.2 Qualitative


#### 5.3 Overfitting Check
\
compare training/validation and testing curves...

### 6. Visual Representation

Dimensionality Reduction Techniques: Sometimes, it's helpful to use dimensionality reduction techniques (like PCA) to identify the most significant variables or components and then focus the GAM analysis and visualization on these.

Visualizing Generalized Additive Models (GAMs) is one of their strong suits, particularly in terms of interpretability. A GAM allows you to see the individual contribution of each predictor to the model, represented through smooth functions. Here's how you can visualize a GAM:

    Partial Dependence Plots: These plots show the relationship between the target variable and each predictor, holding all other predictors constant. For each predictor, the plot displays a smooth curve that represents the effect of that predictor on the response variable. This is particularly useful for understanding non-linear relationships.

    Confidence Intervals: Along with the smooth curve, it's common to include confidence intervals around the estimate. This shows the uncertainty in the model's estimation.

    Individual Conditional Expectation (ICE) Plots: While partial dependence plots show the average effect of a predictor, ICE plots display the relationship for individual observations. This can help in understanding how the model behaves for specific data points.

    Response Plots: For each predictor, you can plot the model's predicted response as a function of that predictor, again keeping other predictors fixed.

### 7. Final Discussion



### 8. References

> <a id="id_1">[1]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm>

> <a id="id_2">[2]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2021/2021_YRBS_Data_Users_Guide_508.pdf>