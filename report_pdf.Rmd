---
title: "Projectreport Machine Learning 2"
header-includes:
  - \usepackage[english]{babel}
  - \DeclareUnicodeCharacter{308}{oe}  
author: "Maluna Menke, Ari (Sara) Wahl, Pavlo Kravets"
date: "`r Sys.Date()`"

output:
  #html_document: 
  pdf_document:
    
    toc: true
    toc_depth: 6
  start-page: 2
---

\pagenumbering{gobble}
\newpage
\pagenumbering{arabic} 
\centering
\raggedright
\newpage

---

```{r setup, include=FALSE}
params <- list(html=knitr::is_html_output())
knitr::opts_chunk$set(echo = params$html)
rm(list = ls(all.names = TRUE)) # reset the environment

#names(RISK)

source("Setup.R")
```

### 1. Introduction

Our general idea was to work with LGBT-related data. This was not as easy as expected, since it seems there are not a lot of datasets openly available that have that kind of information. Finally, we found a US survey by the CDC, that regularly monitors the countries youth in a lot of dimensions, but among other questions also asks for sexual experiences and identification.   

### 2. The Dataset
\
"The *[Youth Risk Behavior Survey (YRBS)](https://www.cdc.gov/healthyyouth/data/yrbs/data.htm)* measures health-related behaviors and experiences that can lead to death and disability among youth and adults.[...] Some of the health-related behaviors and experiences monitored are:
- Student demographics: sex, sexual identity, race and ethnicity, and grade
- Youth health behaviors and conditions: sexual, injury and violence, bullying, diet and physical activity, obesity, and mental health, including suicide
- Substance use behaviors: electronic vapor product and tobacco product use, alcohol use, and other drug use
- Student experiences: parental monitoring, school connectedness, unstable housing, and exposure to community violence [[1]](#1).
It is a national survey conducted by CDC (Center for Disease Control and Prevention) and includes high school students from both private and public schools within the U.S. Data is collected from 1991 through 2021, we are only using the most recent data from 2021 though. If you want to learn more about the data there is an accompanying Data User Guide.[[2]](#2).


#### 2.1 Preprocessing of the dataset
\

```{r summary, include = FALSE}
# possible summaries of the data:
# summary(RISK) 
# describe(RISK) # more detail
```

To preprocess the dataset, we first ran a summary on our dataset. The number of NAs seems to depend very much on the question. The variable "orig_rec" only contained NAs and has therefore been removed, as well as the variable "site" which only contained "XX" entries. Variables q4 and q5 are already aggregated in "raceeth" and have also been deleted.
The variable "record" seems to be an ID for the observations. This has to be considered later. 

#### 2.2 Missing Data

```{r variables overview, include =FALSE}
na_ratio<-sum(is.na(RISK)==TRUE)/(dim(RISK)[1]*dim(RISK)[2])# ratio of NAs among all datapoints
print(na_ratio*100) # % percentage of NAs in data
sum(is.na(RISK)==TRUE) # sum of NAs in dataset
```
\
We will first exclude all the observations with NAs in all the target related variables q25 to q29. Since we want to build our target variable on these questions, the target variable cannot all be empty. The amount of data available should be enough to just exclude these observations. After removing the observations that have NAs in all the variables, that are used to create our target variable, we still have around 13.7% NAs in the dataset.
\
What if we had just excluded every NA in the dataset? We will try and see if this is a viable option, since this would not just be quick and easy, but we would also just have "real" answers. The exclusion of NAs leads to a severe reduction in the number of observations. The original data consisted of 17232 observations, after reducing the target related NAs only, we have 11753 observations left. If we omit all NAs, the reduced dataframe still has 4334 observations.
\

```{r check exclude NAs, include=FALSE}
RISK_no_NAs <- na.omit(RISK)
summary(RISK_no_NAs)
```

In this case need to assess the loss of information foremost in relation to our target variable. The important question is if there is a pattern to the missingness in our data, not just, but especially with regard to our target variable.

```{r Tables for Missingness in q28, include=FALSE}
table(RISK_no_NAs$q28)
table(RISK$q28)

table(RISK_no_NAs$q28)/4334
table(RISK$q28)/17138
sum(is.na(RISK$q28)==TRUE)/17138
```

#### 2.2.2 Omitting NAs vs Data Imputation
\
If we can really omit the NAs or if it may be necessary to impute the missing datapoints, depends on the type of missingness. If data is missing completely at random (MCAR), we can omit the NAs, if it is just missing at random (MAR) we would rather impute the data. If the data is missing not at random (MNAR), it would be a quite difficult problem, because we cannot easily impute the missing data then. To find out if we can just omit the data, an MCAR test was applied. 
\
We test the target related variables q25 to q29 for potential pattern(s) in the missing data. This results in a p-value of 0, which means we can say for sure, that the data is not missing completely at random. Just omitting all NAs could be problematic and lead to bias.
\

```{r MCAR test for target columns}
result <- mcar_test(RISK[, c("q25", "q26", "q27", "q28", "q29")])
print(result)
```
Because of this we will impute the missing data. To ensure a good imputation, we need to impute our NAs before reducing the dataset to 2000 observations. To run the imputation properly we need to factorize our nominal and ordinal variables first.


```{r imputing, include= FALSE}
set.seed(0)
# the categorical variables are factorized in the setup.R file
RISK_imp <- missRanger(RISK, pmm.k = 200, num.trees = 500, sample.fraction = 1)
```


#### 2.3 Target Variable
\
As a target variable, we decided to calculate a score from 5 questions that reflects the suicide risk of the person (observation) in question. This score is aggregated with a rule based approach. 

```{r creating the target variable, include= FALSE}
RISK_imp_num<-RISK_imp
RISK_imp_num$q25 <- as.numeric(RISK_imp_num$q25)
RISK_imp_num$q26 <- as.numeric(RISK_imp_num$q26)
RISK_imp_num$q27 <- as.numeric(RISK_imp_num$q27)
RISK_imp_num$q28 <- as.numeric(RISK_imp_num$q28)
RISK_imp_num$q29 <- as.numeric(RISK_imp_num$q29)
RISK_imp_s5 <- RISK_imp_num %>% 
  mutate(suicidal_class = ifelse(q29 > 1,   5, # 3
                          ifelse(q28 > 1,   5, # 3
                          ifelse(q27 == 1,  4, # 3
                          ifelse(q26 == 1,  3, # 2
                          ifelse(q25 == 1,  2,
                                            1))))))

RISK_imp_s5[, c("q25", "q26", "q27", "q28", "q29")] <- NULL  # removing the columns related to the target variable
```

\ 
After creating the target variable we need to exclude the variables q25 to q29, that were used for creating it, from our dataset. 



#### 2.4 Reducing and balancing the dataset to 2000 observations
\
Now we need to reduce our data to the maximum allowed size of 2000 observations. To ensure the best possible data quality, we want to ensure that our dataset is balanced. Intuitively, we are considering if it is best to still use as much of the non-imputed data for our smaller dataset as possible, before filling it up with imputed data, since non-imputed data is usually of better quality. On the other hand the data seemingly shows patterns in the missingness so there are reasons to just do a stratified sampling over the imputed data as well. To do a proper statified sampling we need to identify the stratification variables. Therefore we will calculate the correlations with the target variable and see which variables are highly correlated to our target variable. These will then as well as the target variable be used as stratification variables.

```{r stratified sampling, include = FALSE}
# this code snipped was produced with the help of AI
# *
# preprocessing for stratification:
# RISK_num <- as.data.frame(lapply(RISK_imp_s, as.numeric)) # convert all variables to numeric
# corr_mat=cor(RISK_num, method="s") # correlation matrix
# target_corr <- corr_mat[, "suicidal_class"] # get correlations with respect to the target variable
# df with variable names and their correlation values:
# corr_data <- data.frame(
#  var = names(target_corr),
#  corr = target_corr
#)

# Sort the data frame by correlation values
# sorted_corr_data <- corr_data[order(-corr_data$corr), ]

# stratification variable vector:
# strat_vars <- sorted_corr_data[abs(sorted_corr_data$corr) > 0.25,] 
# select all correlations as possible strata (pos & neg) bigger than 0.25

# all possible stratum columns
# strat_df<-RISK_imp_s[, strat_vars$var] 

# replicate stratum size
# stratum_sizes <- rep(2000, length(strat_vars$var))

# only use target as stratum:
strat_df<-RISK_imp_s5[, "suicidal_class"]

# sample for 2000 observations:
sample_obj <- strata(strat_df[order("suicidal_class")], size = 2000, method = "srswor")

# draw the stratified sample
RISK_2000_5 <- getdata(RISK_imp_s5, sample_obj)
# *
# clean up:
# rm()  # remove unnecessary variables in the environment

```


#### 2.5 Simple Synopsis of the Dataset

```{r study overview, include=FALSE}
# str(RISK) # 
variable_types <- skim(RISK_imp) # to info for get the variables
summary(variable_types) # summary on variable types
summary(variable_types$factor.ordered) # summary on factor variables
```
\
number of observations:
number of variables:

datatypes:
- nominal variables:
- ordinal variables:
(numeric variables:)
- discrete variables:
- continuous variables:



### 3. Additional Data Preparation

#### 3.1 Feature Reduction
\
Since our dataset has lots of variables, we decided to start by excluding some variables depending on the estimated feature importance.

#### 3.1.1 Correlations 


#### 3.1.2 Feature Importance Algorithm



#### 3.2 Naming the variables and the factor levels
\
This is an optional step. It can lead to a better readability of our data frame though.

```{r naming variables, include=FALSE}
# source("names.R") 

# todo finish the naming for all variables...
# maybe only after the feature reduction step
```

For an easy understanding of their values, the variables and levels are named according to their content.


#### 3.3 Splitting the Data
\
According to the project requirements we split our data in 60% Training, 20% Validation and 20% Testing Data. 

```{r, corr plot, echo=FALSE}

#pairs(RISK) # -> non numeric arguments included
#plot(RISK) 

# corr_mat=cor(RISK, method="s") #create Spearman correlation matrix

#corrplot(corr_mat, method = "color",
#     type = "upper", order = "hclust", 
#     tl.col = "black") 
```





### 4. Machine Learning Models

#### 4.1 Short Mathematical Overview on the used Methods



#### 4.2 Fitting process



#### 4.3 Hyperparameter Optimization



```{r , include=FALSE}

```


```{r print model with stargazer, echo=FALSE}

# stargazer(model, type = "text") # Modell ausgeben mit stargazer
```


\


```{r Signifikanz der Modelle, include= FALSE}
# lrtest(model_best) # erarbeitetes Modell ist besser als Nullmodell?
```

Mithilfe eines Likelihood-Ratio Tests wird überprüft welches Modell das bessere ist.
```{r Vergleich der Modelle, echo=FALSE}

# lrtest(model_best, model) # restringiertes Modell und optimiertes Modell mit mehr IA-Termen

```
Das optimierte Modell ist signifikant besser als das erarbeitete zum festgelegten Signifikanzniveau,
die Nullhypothese des LR-Tests, dass das optimierte Modell nicht besser ist als das komplexere, kann verworfen werden. Wir stellen außerdem fest: Beide Modelle weisen einen signifikanten Erklärungsgehalt auf, sie sind besser als das Nullmodell.

### 5. Comparison of the Models / Model's Performance on Test Data


#### 5.1 Quantitative 

##### 5.1.1 Confusion Matrix

##### 5.1.2 Accuracy

##### 5.1.2 Precision, Recall and F1-Score

#### 5.2 Qualitative


#### 5.3 Overfitting Check

### 6. Visual Representation

### 7. Final Discussion



### 8. References

> <a id="id_1">[1]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/overview.htm>

> <a id="id_2">[2]</a> <https://www.cdc.gov/healthyyouth/data/yrbs/pdf/2021/2021_YRBS_Data_Users_Guide_508.pdf>